# -*- coding: utf-8 -*-
"""CS Labs detyra 22-01.ipynb

Automatically generated by Colab.
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import time
from urllib.parse import urljoin
from datetime import datetime
from os import name

homepage_url = "http://books.toscrape.com/"
homepage_response = requests.get(homepage_url)
homepage_response.encoding = 'utf-8' #per ruajtjen e cmimit
homepage_soup = BeautifulSoup(homepage_response.text, 'html.parser')

# getting a category list with the category name and the corresponding link

def get_categories():
  response = requests.get(homepage_url)
  response.encoding = 'utf-8'
  soup = BeautifulSoup(response.text, 'html.parser')
  categories_tags = soup.select_one('div.side_categories > ul > li > ul').find_all('li', recursive = False)
  categories = {}
  for tag in categories_tags:
    name = tag.text.strip()
    link = urljoin(homepage_url, tag.find('a')['href'])
    categories[name] = link
  return categories

books={}

#get all books from all pages of a specific category
def scrape_category(category_name, start_url):
  current_url = start_url

  while current_url:
    response = requests.get(current_url)
    response.encoding = 'utf-8'
    soup = BeautifulSoup(response.text, 'html.parser')

    for article in soup.find_all('article', class_='product_pod'):
      title = article.h3.a['title']
      #if book is already scraped, then we add the other category
      if title in books:
        books[title]['Categories'].add(category_name)
      else:
        price = article.find('p', class_='price_color').text
        clean_price = float(re.findall(r'[0-9.]+', price)[0])
        #adds all books of the current page
        books[title] = {
            'Title': title,
            'Price (GBP)': clean_price,
            'Categories': {category_name}
        }
# also checking for pagination, if there's other pages, we scrape those too
    next_button = soup.find('li', class_='next')
    if next_button:
      relative_link = next_button.find('a')['href']
      current_url = urljoin(current_url, relative_link)
    else:
      current_url = None

  return books

all_categories = get_categories()
for category_name, link in all_categories.items():
  print(f"Scraping category: {category_name}")
  scrape_category(category_name, link)

def get_exchange_rate(api_key, base_currency, target_currency):
  url = f"https://v6.exchangerate-api.com/v6/{api_key}/latest/{base_currency}"
  response = requests.get(url)
  data = response.json()

  if data["result"] == "success":
    return data["conversion_rates"][target_currency]
  else:
    print("Error fetching API data!")
    return None

API_KEY = ""

rate = get_exchange_rate(API_KEY, "GBP", "EUR")

print("Exchange Rate GBP->EUR: ", rate)

# Manual test: Add 'Test Category' to the first book found
first_book_title = list(books.keys())[0]
books[first_book_title]['Categories'].add('Test Category')

final_list = list(books.values())
for item in final_list:
  item['Categories'] = ', '.join(sorted(item['Categories']))

df = pd.DataFrame(final_list)

#shtohet kolona per ID
df.insert(0, 'ID', range(1, 1 + len(df)))

#shtohet kolona per cmimin ne EUR
df['Price (EUR)'] = (df['Price (GBP)'] * rate).round(2)

#shtohet kolona per daten e kembimit
df['Exchange Date'] = datetime.now().strftime("%Y-%m-%d")

#krijohet file .csv
df.to_csv('books_data.csv', index=False, encoding='utf-8')
print(df)


